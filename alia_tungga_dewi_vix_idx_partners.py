# -*- coding: utf-8 -*-
"""Alia Tungga Dewi - VIX - IDX Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CtP_EWkqTRiaol8veSGja1SoMaqebGBF

# Import libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt 

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('loan_data_2007_2014.csv')

"""# Data Descriprion"""

df.info()

df.describe()

df.head()

df.tail()

df.columns

"""# Assigning the target column to identify"""

#create a new column based on the loan_status
df['bad_loan'] = np.where(df.loc[:,'loan_status'].isin(['Charged Off', 'Default', 'Late (31-120 days)',
                                    'Does not meet the credit policy. Status:Charged Off']), 0, 1)

#drop theoriginal loan_staus
df.drop(columns=['loan_status'],inplace=True)
df.drop('Unnamed: 0', inplace=True, axis=1)

X = df.drop('bad_loan', axis = 1)
y = df['bad_loan']

#cols that have >70% missing values
missing_values = df.isnull().mean()

missing_values[missing_values>0.7]

"""# Data Preprocessing"""

columns_to_drop = ['id', 'member_id', 'sub_grade', 'emp_title', 'url', 'desc', 'title', 'zip_code', 'next_pymnt_d',
                  'recoveries', 'collection_recovery_fee', 'total_rec_prncp', 'total_rec_late_fee', 'desc', 'mths_since_last_record',
                  'mths_since_last_major_derog', 'annual_inc_joint', 'dti_joint', 'verification_status_joint', 'open_acc_6m', 'open_il_6m',
                  'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il', 'il_util', 'open_rv_12m', 'open_rv_24m',
                  'max_bal_bc', 'all_util', 'inq_fi', 'total_cu_tl', 'inq_last_12m','policy_code',]
df.drop(columns=columns_to_drop, inplace=True, axis=1)

df.dropna(inplace=True)

#cheking correlation among features

mask = np.zeros_like(df.corr().fillna(0), dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
plt.figure(figsize=(24,24))
sns.heatmap(df.corr(), mask=mask, annot=True,  cmap="YlGnBu", vmin = -1, fmt='.1g', edgecolor='w', linewidth=0.6)

#Removing multicolliear features

df.drop(columns=['loan_amnt', 'revol_bal', 'funded_amnt', 'funded_amnt_inv', 'installment',  
                   'total_pymnt_inv',  'out_prncp_inv',  'total_acc'], inplace=True)

mask = np.zeros_like(df.corr().fillna(0), dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
plt.figure(figsize=(24,24))
sns.heatmap(df.corr(), mask=mask, annot=True,  cmap="magma", vmin = -1, fmt='.1g', edgecolor='w', linewidth=0.6)

def date_columns(df_data, column):
    # store current month
    today_date = pd.to_datetime('2020-08-01')
    # convert to datetime format
    df_data[column] = pd.to_datetime(df_data[column], format = "%b-%y")
     # calculate the difference in months and add to a new column
    df_data['mths_since_' + column] = round(pd.to_numeric((today_date - df_data[column]) / np.timedelta64(1, 'M')))
    # make any resulting -ve values to be equal to the max date
    df_data['mths_since_' + column] = df_data['mths_since_' + column].apply(lambda x: df_data['mths_since_' + column].max() if x < 0 else x)
    # drop the original date column
    df_data.drop(columns = [column], inplace = True)
    
backup_data = df
preprocess_data = df

# Convert categorical features to continuous features with Label Encoding
from sklearn.preprocessing import LabelEncoder
lencoders = {}
for col in preprocess_data.select_dtypes(include=['object']).columns:
    lencoders[col] = LabelEncoder()
    preprocess_data[col] = lencoders[col].fit_transform(preprocess_data[col])

#seperating data into target and features
X= preprocess_data.drop(columns='bad_loan', axis=1)
y=preprocess_data['bad_loan']

import gc
gc.collect()

"""# Data type transformation"""

def term_numeric(df_data, column):
    df_data[column] = pd.to_numeric(df[column].str.replace(' months', ''))
    term_numeric(df, 'term')

missing_values = df.isnull().sum()
missing_values[missing_values>0]/len(df)

categorical_features = df.select_dtypes(exclude='number')
numerical_features = df.select_dtypes(exclude='object')

filled_data = df
preprocess_data = df

missing = preprocess_data.isnull().sum()
missing[missing>0]

"""# Binning, Weight of Evidence (WoE), Information Value (IV)"""

def iv_woe(df, target, bins=10, show_woe=False):
    
    newDF,woeDF = pd.DataFrame(), pd.DataFrame()
    cols = df.columns
    
    #Run WoE and IV on all the independent variables
    for ivars in cols[~cols.isin([target])]:
        if (df[ivars].dtype.kind in 'bifc') and (len(np.unique(df[ivars]))>10):
            binned_x = pd.qcut(df[ivars], bins,  duplicates='drop')
            d0 = pd.DataFrame({'x': binned_x, 'y': df[target]})
        else:
            d0 = pd.DataFrame({'x': df[ivars], 'y': df[target]})
        d = d0.groupby("x", as_index=False).agg({"y": ["count", "sum"]})
        d.columns = ['Cutoff', 'N', 'Events']
        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()
        d['Non-Events'] = d['N'] - d['Events']
        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()
        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])
        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])
        d.insert(loc=0, column='Variable', value=ivars)
        print("Information value of " + ivars + " is " + str(round(d['IV'].sum(),6)))
        temp =pd.DataFrame({"Variable" : [ivars], "IV" : [d['IV'].sum()]}, columns = ["Variable", "IV"])
        newDF=pd.concat([newDF,temp], axis=0)
        woeDF=pd.concat([woeDF,d], axis=0)

        if show_woe == True:
            print(d)
            
    return newDF, woeDF

iv, woe = iv_woe(preprocess_data, target='bad_loan', bins=20)

data_dummies1 = [pd.get_dummies(preprocess_data['grade'], prefix='grade', prefix_sep=':'),
                 pd.get_dummies(preprocess_data['home_ownership'], prefix='home_ownership', prefix_sep=':'),
                 pd.get_dummies(preprocess_data['verification_status'], prefix='verification_status', prefix_sep=':'),
                 pd.get_dummies(preprocess_data['purpose'], prefix='purpose', prefix_sep=':'),
                 pd.get_dummies(preprocess_data['addr_state'], prefix='addr_state', prefix_sep=':'),
                 pd.get_dummies(preprocess_data['initial_list_status'], prefix='initial_list_status', prefix_sep=':')
                ]

categorical_dummies = pd.concat(data_dummies1, axis=1)
preprocess_data = pd.concat([preprocess_data, categorical_dummies], axis=1)

# Plot WoE

def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):
    x = np.array(df_WoE.iloc[:, 0].apply(str))
    y = df_WoE['WoE']
    plt.figure(figsize=(18, 12))
    plt.plot(x, y, marker = 'o', color = 'green', linestyle = 'dashed', linewidth = 4 , markersize = 20, markeredgecolor = 'cyan', markerfacecolor = 'black')
    plt.xlabel(df_WoE.columns[0])
    plt.ylabel('Weight of Evidence')
    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))
    plt.xticks(rotation = rotation_of_x_axis_labels) 
    plt.xticks(fontsize=16)
    plt.yticks(fontsize=16)
    plt.show()

#function to calculate WoE of cat features

def woe_categorical(df_data, cat_feature, good_bad_df):
    df_data = pd.concat([df_data[cat_feature], good_bad_df], axis=1)
    df_data = pd.concat([df_data.groupby(df_data.columns.values[0], as_index = False)[df_data.columns.values[1]].count(),
                    df_data.groupby(df_data.columns.values[0], as_index = False)[df_data.columns.values[1]].mean()], axis = 1)
    df_data = df_data.iloc[:, [0, 1, 3]]
    df_data.columns = [df_data.columns.values[0], 'n_obs', 'prop_good']
    df_data['prop_n_obs'] = df_data['n_obs'] / df_data['n_obs'].sum()
    df_data['n_good'] = df_data['prop_good'] * df_data['n_obs']
    df_data['n_bad'] = (1 - df_data['prop_good']) * df_data['n_obs']
    df_data['prop_n_good'] = df_data['n_good'] / df_data['n_good'].sum()
    df_data['prop_n_bad'] = df_data['n_bad'] / df_data['n_bad'].sum()
    df_data['WoE'] = np.log(df_data['prop_n_good'] / df_data['prop_n_bad'])
    df_data = df_data.sort_values(['WoE'])
    df_data = df_data.reset_index(drop = True)
    df_data['diff_prop_good'] = df_data['prop_good'].diff().abs()
    df_data['diff_WoE'] = df_data['WoE'].diff().abs()
    df_data['IV'] = (df_data['prop_n_good'] - df_data['prop_n_bad']) * df_data['WoE']
    df_data['IV'] = df_data['IV'].sum()
    return df_data

X= preprocess_data.drop(columns='bad_loan', axis=1)
y=preprocess_data['bad_loan']

"""# Analyzing Categorical Variables by Plotting WoE

1. Analyzing grade variable
"""

df_grade = woe_categorical(X, 'grade', y)
plot_by_woe(df_grade)

"""2. Analyzing Verification status variable"""

veri_df = woe_categorical(X, 'verification_status', y)
plot_by_woe(veri_df)

"""3. Analyzing Purpose variable"""

pur_df = woe_categorical(X, 'purpose', y)
plot_by_woe(pur_df, 90)

"""# Analyzing Continous Variables by Plotting WoE"""

def woe_continous(df_data, cat_feature, good_bad_df):
    df_data = pd.concat([df_data[cat_feature], good_bad_df], axis=1)
    df_data = pd.concat([df_data.groupby(df_data.columns.values[0], as_index = False)[df_data.columns.values[1]].count(),
                    df_data.groupby(df_data.columns.values[0], as_index = False)[df_data.columns.values[1]].mean()], axis = 1)
    df_data = df_data.iloc[:, [0, 1, 3]]
    df_data.columns = [df_data.columns.values[0], 'n_obs', 'prop_good']
    df_data['prop_n_obs'] = df_data['n_obs'] / df_data['n_obs'].sum()
    df_data['n_good'] = df_data['prop_good'] * df_data['n_obs']
    df_data['n_bad'] = (1 - df_data['prop_good']) * df_data['n_obs']
    df_data['prop_n_good'] = df_data['n_good'] / df_data['n_good'].sum()
    df_data['prop_n_bad'] = df_data['n_bad'] / df_data['n_bad'].sum()
    df_data['WoE'] = np.log(df_data['prop_n_good'] / df_data['prop_n_bad'])
    df_data['diff_prop_good'] = df_data['prop_good'].diff().abs()
    df_data['diff_WoE'] = df_data['WoE'].diff().abs()
    df_data['IV'] = (df_data['prop_n_good'] - df_data['prop_n_bad']) * df_data['WoE']
    df_data['IV'] = df_data['IV'].sum()
    return df_data

"""1. Analyzing total rec int variable"""

X['total_rec_int_factor'] = pd.cut(X['total_rec_int'], 20)
rec_int_df = woe_continous(X, 'total_rec_int_factor', y)
plot_by_woe(rec_int_df, 90)

"""2. Analyzing toal revolving_high_limit variable"""

X['total_rev_hi_lim_factor'] = pd.cut(X['total_rev_hi_lim'], 100)
revol_hi_df = woe_continous(X, 'total_rev_hi_lim_factor', y)
#plot_by_woe(revol_hi_df, 90)

#analyzing below 100000
X_train_prepr_temp = X[X['total_rev_hi_lim'] <= 100000].copy()
# fine-classing
X_train_prepr_temp['total_rev_hi_lim_factor'] = pd.cut(X_train_prepr_temp['total_rev_hi_lim'],10)
# select only the relevant index in the target col
df_temp = woe_continous(X_train_prepr_temp, 'total_rev_hi_lim_factor', y[X_train_prepr_temp.index])
plot_by_woe(df_temp, 90)

"""3. Analyzing total payment variable """

X['total_pymnt_factor'] = pd.cut(X['total_pymnt'], 10)
total_pym_df = woe_continous(X, 'total_pymnt_factor', y)
plot_by_woe(total_pym_df)

"""4. Analyzing dti variable"""

X['dti_factor'] = pd.cut(X['dti'], 10)
dti_df = woe_continous(X, 'dti_factor', y)
plot_by_woe(dti_df)

"""5. Analyzing annual income variable"""

X['annual_inc_factor'] = pd.cut(X['annual_inc'], 50)
ann_inc_df = woe_continous(X, 'annual_inc_factor', y)
plot_by_woe(ann_inc_df, 90)

"""6. Analyzing total current balance variable"""

#Analyzing current balance  below 400000
X_train_prepr_temp = X[X['tot_cur_bal'] <= 400000].copy()
# fine-classing
X_train_prepr_temp['tot_cur_bal_factor'] = pd.cut(X_train_prepr_temp['tot_cur_bal'], 10)
# select only the relevant index in the target column
df_temp = woe_continous(X_train_prepr_temp, 'tot_cur_bal_factor', y[X_train_prepr_temp.index])
plot_by_woe(df_temp, 90)